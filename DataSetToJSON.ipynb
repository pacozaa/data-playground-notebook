{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcvrm3k3RewmeUQ2BQiyE5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pacozaa/data-playground-notebook/blob/main/DataSetToJSON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0w9F0iISKi7",
        "outputId": "18551cd3-6c18-49db-9f84-e74af9d34568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# Download the Punkt tokenizer data (only needed once)\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSD9clh40Goc",
        "outputId": "b3853bd5-3183-431a-9aec-42f9f9bb5fb5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def combine_words(text, x):\n",
        "    split_words = word_tokenize(text)\n",
        "    combined_words = []\n",
        "\n",
        "    # Iterate over the split words in steps of x\n",
        "    for i in range(0, len(split_words), x):\n",
        "        # Combine x words into a single string\n",
        "        combined_word = ' '.join(split_words[i:i+x])\n",
        "        combined_words.append(combined_word)\n",
        "\n",
        "    return combined_words\n",
        "\n",
        "def combine_words_from_split_words(split_words, x):\n",
        "    combined_words = []\n",
        "\n",
        "    # Iterate over the split words in steps of x\n",
        "    for i in range(0, len(split_words), x):\n",
        "        # Combine x words into a single string\n",
        "        combined_word = ' '.join(split_words[i:i+x])\n",
        "        combined_words.append(combined_word)\n",
        "\n",
        "    return combined_words\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a sample text to demonstrate the combination of words.\"\n",
        "x = 3  # Number of words to combine\n",
        "combined_words = combine_words(text, x)\n",
        "print(combined_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPcpk7UTwgg7",
        "outputId": "2d892889-83c2-4bec-fb8f-abec2bbe0d5e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a', 'sample text to', 'demonstrate the combination', 'of words .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def saveToFile(name,dataset_name,data):\n",
        "  dataWithId= [\n",
        "      {\"content\": item, \"id\": str(uuid.uuid4())}\n",
        "      for item in data\n",
        "  ]\n",
        "  json_array = json.dumps(dataWithId, indent=2)\n",
        "  file_name = f\"\"\"{name}-{len(dataWithId)}-{dataset_name.replace(\"/\", \"-\")}.json\"\"\"\n",
        "  with open(file_name, \"w\") as f:\n",
        "      f.write(json_array)"
      ],
      "metadata": {
        "id": "oABvFO9u1psa"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import uuid\n",
        "import random\n",
        "def extract_sentences(dataset,sample_size,dataset_name,column_name):\n",
        "  # Initialize an empty list to store the sentences\n",
        "  sentences = []\n",
        "  # Initialize an empty list to store the words\n",
        "  words = []\n",
        "\n",
        "  # Iterate over the dataset and split the text into sentences\n",
        "  for example in dataset[\"train\"].shuffle().select(range(200)):\n",
        "      text = example[column_name]\n",
        "      # SENTENCE\n",
        "      # Split the text into sentences\n",
        "      split_sentences = sent_tokenize(text)\n",
        "      # Extend the list with the split sentences\n",
        "      sentences.extend(split_sentences)\n",
        "\n",
        "      # WORD\n",
        "      # Split the text into words\n",
        "      split_words = word_tokenize(text)\n",
        "      combined_words = combine_words_from_split_words(split_words, 5)\n",
        "      # Extend the list with the split words\n",
        "      words.extend(combined_words)\n",
        "\n",
        "  #Filter words\n",
        "  words=[word for word in words if any(c.isalnum() for c in word)]\n",
        "\n",
        "\n",
        "\n",
        "  # Shuffle the sentences and words list\n",
        "  random.shuffle(sentences)\n",
        "  random.shuffle(words)\n",
        "  mix=sentences+words\n",
        "  random.shuffle(mix)\n",
        "  # Truncate the sentences list to the first 50 sentences\n",
        "  sentences = sentences[:sample_size]\n",
        "  words = words[:sample_size]\n",
        "  mix = mix[:sample_size]\n",
        "\n",
        "  # Optionally, save the JSON array to a file\n",
        "  saveToFile(\"sentence\",dataset_name,sentences)\n",
        "  saveToFile(\"word\",dataset_name,words)\n",
        "  saveToFile(\"mix\",dataset_name,mix)\n",
        "  return sentences, words, mix"
      ],
      "metadata": {
        "collapsed": true,
        "id": "97fXZYrySNGR"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset_list = [\n",
        "    {\n",
        "        \"name\": \"imdb\",\n",
        "        \"column\": \"text\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"sentence-transformers/stsb\",\n",
        "        \"column\": \"sentence1\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"roneneldan/TinyStories\",\n",
        "        \"column\": \"text\"\n",
        "    },\n",
        "    # {\n",
        "    #     \"name\": \"derek-thomas/ScienceQA\",\n",
        "    #     \"column\": \"question\"\n",
        "    # },\n",
        "    {\n",
        "        \"name\": \"derek-thomas/dataset-creator-reddit-bestofredditorupdates\",\n",
        "        \"column\": \"content\"\n",
        "    },\n",
        "    {\n",
        "        \"name\":\"LIDIA-HESSEN/vencortex-BusinessNewsDataset\",\n",
        "        \"column\":\"text\"\n",
        "    },\n",
        "    {\n",
        "        \"name\":\"yashraizad/yelp-open-dataset-business\",\n",
        "        \"column\":\"categories\"\n",
        "    },\n",
        "    {\n",
        "        \"name\":\"yashraizad/top-reviews-per-business\",\n",
        "        \"column\":\"text\"\n",
        "    },\n",
        "    {\n",
        "        \"name\":\"warrencain/Business_Knowledge_Dataset_Llama_3.1_Medium_Token_Pairs\",\n",
        "        \"column\":\"Output\"\n",
        "    },\n",
        "    {\n",
        "        \"name\":\"darrel999/business-java-code\",\n",
        "        \"column\":\"output\"\n",
        "    }\n",
        "]\n",
        "\n",
        "all_sentences=[]\n",
        "all_words=[]\n",
        "all_mix=[]\n",
        "sample_size=50\n",
        "# Loop throught dataset_name_list and run extract_sentences(dataset)\n",
        "for dataset_data in dataset_list:\n",
        "  print(f\"\"\"Loading.....{dataset_data[\"name\"]}\"\"\")\n",
        "  # Load Dataset\n",
        "  dataset_name=dataset_data[\"name\"]\n",
        "  dataset = load_dataset(dataset_name)\n",
        "  print(f\"\"\"data length: {len(dataset[\"train\"])}\"\"\")\n",
        "  column_name=dataset_data[\"column\"]\n",
        "  # Print all the column names in the dataset\n",
        "  # print(dataset[\"train\"].column_names)\n",
        "\n",
        "  sentences, words, mix = extract_sentences(dataset,sample_size,dataset_name,column_name)\n",
        "  all_sentences = all_sentences + sentences\n",
        "  all_words = all_words + words\n",
        "  all_mix = all_mix + mix\n",
        "\n",
        "random.shuffle(all_sentences)\n",
        "random.shuffle(all_words)\n",
        "random.shuffle(all_mix)\n",
        "print(f\"\"\"Total sentences: {len(all_sentences)}\"\"\")\n",
        "print(f\"\"\"Total words: {len(all_words)}\"\"\")\n",
        "print(f\"\"\"Total mix: {len(all_mix)}\"\"\")\n",
        "# Truncate the sentences list to the first 50 sentences\n",
        "all_sentences = all_sentences[:50]\n",
        "all_words = all_words[:150]\n",
        "all_mix = all_mix[:100]\n",
        "saveToFile(\"all_sentences\",\"\",all_sentences)\n",
        "saveToFile(\"all_words\",\"\",all_words)\n",
        "saveToFile(\"all_mix\",\"\",all_mix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrv8K8Y7vug4",
        "outputId": "7d8bd8fb-5fc7-4124-e078-7386bfa48be6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading.....imdb\n",
            "data length: 25000\n",
            "Loading.....sentence-transformers/stsb\n",
            "data length: 5749\n",
            "Loading.....roneneldan/TinyStories\n",
            "data length: 2119719\n",
            "Loading.....derek-thomas/dataset-creator-reddit-bestofredditorupdates\n",
            "data length: 11595\n",
            "Loading.....LIDIA-HESSEN/vencortex-BusinessNewsDataset\n",
            "data length: 469361\n",
            "Loading.....yashraizad/yelp-open-dataset-business\n",
            "data length: 150346\n",
            "Loading.....yashraizad/top-reviews-per-business\n",
            "data length: 53340\n",
            "Loading.....warrencain/Business_Knowledge_Dataset_Llama_3.1_Medium_Token_Pairs\n",
            "data length: 29362\n",
            "Loading.....darrel999/business-java-code\n",
            "data length: 53183\n",
            "Total sentences: 450\n",
            "Total words: 450\n",
            "Total mix: 450\n"
          ]
        }
      ]
    }
  ]
}